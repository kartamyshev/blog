{"pageProps":{"post":{"id":11,"contentHTML":"<h3>The history of typeof null</h3>\n<blockquote>\n<p>The <a href=\"http://mxr.mozilla.org/classic/source/js/src/jsapi.h\">first JavaScript engine</a>\nrepresented JavaScript values as 32-bit words.\nThe lowest 3 bits of such a word were used as a type tag, to indicate whether\nthe value was an object, an integer, a double, a string, or a boolean\n(as you can see, even this early engine already stored numbers as integers if possible).</p>\n<p>The type tag for objects was 000. In order to represent the value null, the engine used the\nmachine language NULL pointer, a word where all bits are zero. typeof checked the\ntype tag to determine the type of value, which is why it reported null to be an object.</p>\n</blockquote>\n<h3>History: Why are objects always truthy?</h3>\n<blockquote>\n<p>The conversion to boolean is different for historic reasons. For ECMAScript 1,\nit was decided to not enable objects to configure that conversion (e.g., via a toBoolean() method).\nThe rationale was that the boolean operators || and &#x26;&#x26; preserve the values of their operands.\nTherefore, if you chain those operators, the same value may be checked multiple times for\ntruthiness or falsiness. Such checks are cheap for primitives, but would be costly for\nobjects if they were able to configure their conversion to boolean. ECMAScript\n1 avoided that cost by making objects always truthy.</p>\n</blockquote>\n","title":"Useful quotations from \"Speaking Javascript\" book ","date":"2015-03-22 09:46:42"}},"__N_SSG":true}